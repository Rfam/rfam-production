"""
Copyright [2009-2016] EMBL-European Bioinformatics Institute
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
     http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""

# -----------------------------------IMPORTS-----------------------------------

import json
import os
import subprocess

from config import gen_config as gc
from config import rfam_config as rc
from scripts.export.genomes import genome_fetch as gf


# -----------------------------------------------------------------------------


def domain_download_validator(domain_dir, filename=None):
    """
    Lists all proteome directories in dest_dir and creates a list
    of the genomes that were not downloaded successfully. If filename
    is provided then the Upids will be listed in filename.list

    domain_dir: Destination directory, could be one of the four domains
    filename: The filename for the UPID list/ validation report
    returns: None if filename is provided otherwise it will return a list
    of upids
    """

    recovery_list = []

    updirs = os.listdir(domain_dir)

    for updir in updirs:
        lsf_output_file = os.path.join(domain_dir, os.path.join(updir, "download.out"))
        status = check_genome_download_status(lsf_output_file)
        if status == 0:
            recovery_list.append(updir)

    if filename is not None:
        recovery_file = open(os.path.join(domain_dir, filename + '.list'))
        for upid in recovery_list:
            recovery_file.write(upid + '\n')
        recovery_file.close()

    else:
        return recovery_list

    return None


# -----------------------------------------------------------------------------


def check_genome_download_status(lsf_out_file):
    """
    Opens LSF output file and checks whether the job's status is success

    lsf_out_file: LSF platform's output file generated by -o option
    returns: status 1 if the download was successful, otherwise 0
    """

    infile_fp = open(lsf_out_file, 'r')

    status = 0

    for line in infile_fp:
        if line.find("Success") != -1:
            status = 1

    infile_fp.close()

    return status


# -----------------------------------------------------------------------------


def project_download_validator(project_dir, id_pairs_file=None, filename=None):
    """
    Loops over a genome download project directory and reports all the upids
    that need to be recovered

    project_dir: Destination directory of genome download pipeline
    id_pairs_file: A json file with all the UPids of the corresponding
    Uniprot's release. If None simply reports a list of UPIds
    filename: A name for the output file. "recovery.tsv" will be used otherwise

    returns: void
    """
    upids_to_recover = []

    sub_dirs = [x for x in os.listdir(project_dir) if x in gc.DOMAINS]

    for sub_dir in sub_dirs:
        domain_dir_path = os.path.join(project_dir, sub_dir)
        upids_to_recover.extend(domain_download_validator(domain_dir_path,
                                                          filename=None))
        # would also be good to remove those genome dirs

    if filename is None:
        filename = "recovery"

    if len(upids_to_recover) != 0:
        fp_out = open(os.path.join(project_dir, filename + '.tsv'), 'w')

        if id_pairs_file is not None:
            fp_in = open(id_pairs_file, 'r')
            all_id_pairs = json.load(fp_in)
            fp_in.close()

            for upid in upids_to_recover:
                fp_out.write(upid + '\t' + str(all_id_pairs[upid]["GCA"]) +
                             '\t' + all_id_pairs[upid]["DOM"] + '\n')

        # list upids if the UPID/GCA pairs are not available
        else:
            for upid in upids_to_recover:
                fp_out.write(upid + '\n')

        fp_out.close()

    else:
        print "\nGenome Download Success!"


# -----------------------------------------------------------------------------

def get_empty_file_accessions(domain_dir):
    """
    Loops over all genome directories under the main domain directory and looks
    for any empty files. If there are any, it will create a json file with all
    the missing file accessions and prints out whether download was a Success
    or Failure

    domain_dir: The path to a domain directory (e.g. bacteria) where all files
    have been downloaded

    returns: void
    """

    empty_file_accessions = {}
    upid_dirs = [x for x in os.listdir(domain_dir) if os.path.isdir(os.path.join(domain_dir, x))]

    for upid in upid_dirs:
        empty_files = []
        updir_loc = os.path.join(domain_dir, upid)

        genome_files = [x for x in os.listdir(os.path.join(domain_dir, upid)) if
                        x.endswith(".fa") and x.find("UP") == -1]

        for gen_file in genome_files:
            gen_file_path = os.path.join(updir_loc, gen_file)
            if os.path.getsize(gen_file_path) == 0:
                empty_files.append(gen_file.split(".fa")[0])

        if len(empty_files) > 0:
            empty_file_accessions[upid] = empty_files

    if len(empty_file_accessions.keys()) > 0:
        filename = os.path.split(domain_dir)[1]
        fp_out = open(os.path.join(domain_dir, filename + "_empty_files.json"), 'w')
        json.dump(empty_file_accessions, fp_out)
        fp_out.close()


# -----------------------------------------------------------------------------
def check_all_genome_files_exist(project_dir, upid_gca_file=None):
    """
    This function will extract all accessions per genome and check that all files
    exist. A json file will be generated with all missing accessions so that they
    can be downloaded using restore_gen_download. It reports download status for
    all domain subdirectories and mark it as "Success" or "Failure". In case of
    failure it will generate a json file with all accessions to restore

    project_dir: A project directory as generated by genome_downloader pipeline
    upid_gca_file: All upid-gca pairs either in json or tsv format. None by
    default. If None it will use the json file produced in project_dir during
    genome download
    """

    # list all domain directories in project
    domain_dirs = [x for x in os.listdir(project_dir) if x in gc.DOMAINS]
    upid_gca_pairs = None

    # load upid_gca pairs from project directory (.json)
    if upid_gca_file is None:
        upid_gca_fp = open(os.path.join(project_dir, "upid_gca_dict.json"), 'r')
        upid_gca_pairs = json.load(upid_gca_fp)
        upid_gca_fp.close()

    for domain in domain_dirs:
        domain_missing_accs = {}
        domain_dir_loc = os.path.join(project_dir, domain)

        upids = [x for x in os.listdir(domain_dir_loc)
                 if os.path.isdir(os.path.join(domain_dir_loc, x))]

        gen_missing_accs = []
        for upid in upids:
            upid_dir = os.path.join(domain_dir_loc, upid)

            gca_acc = upid_gca_pairs[upid]["GCA"]
            accessions = gf.fetch_genome_accessions(upid, gca_acc)

            # assuming that all files are decompressed and should be to avoid problems
            for accession in accessions:
                if accession is not None:
                    if check_file_status(os.path.join(upid_dir, accession + ".fa")) is False:
                        gen_missing_accs.append(accession)
                else:
                    print upid

            if len(gen_missing_accs) > 0:
                domain_missing_accs[upid] = gen_missing_accs

        if len(domain_missing_accs.keys()) > 0:
            fp_out = open(os.path.join(domain_dir_loc, domain + "acc_recovery.json"), 'w')
            json.dump(domain_missing_accs, fp_out)
            fp_out.close()

            print "%s Validation: Failure" % domain

        else:
            print "%s Validation: Success" % domain


# -----------------------------------------------------------------------------


def check_file_status(seq_file):
    """
    Performs some sanity checks on the sequence file. Checks if file is
    compressed and if not validates the format using esl-seqstat. It will also
    check if the sequence file provided is empty or not

    seq_file: The path to a valid sequence file
    returns: True if file passed validation checks, False otherwise
    """

    status = True

    # compressed file
    if seq_file.endswith(".gz"):
        if not os.path.exists(seq_file):
            return False
        else:
            return check_compressed_file(seq_file)

    # uncompressed fasta format
    elif seq_file.endswith(".fa"):

        # check that file exists
        if not os.path.exists(seq_file):
            return False

        # check content
        else:
            cmd_args = [rc.ESL_SEQSTAT, '--informat', "fasta", seq_file]
            channel = subprocess.Popen(cmd_args, stdout=subprocess.PIPE)

            # fetch esl-seqstat result
            proc_output = channel.communicate()

            # process the response
            esl_seqstat_out = proc_output[0].split('\n')
            # check only first line of response
            if esl_seqstat_out[0].find("Parse failed") != -1 \
                or esl_seqstat_out[0].find("Format") != -1:
                return False

    # check the size of the file
    if os.path.getsize(seq_file) == 0:
        return False

    return status


# -----------------------------------------------------------------------------


def check_compressed_file(filename):
    """
    Checks if the provided file is in one of the compressed formats

    filename: The path to input file
    returns: True if the file is compressed, False otherwise
    """

    magic_dict = {
        "\x1f\x8b\x08": "gz",
        "\x42\x5a\x68": "bz2",
        "\x50\x4b\x03\x04": "zip"
    }

    max_len = max(len(x) for x in magic_dict)

    with open(filename) as fp_in:
        file_start = fp_in.read(max_len)

    for magic, filetype in magic_dict.items():
        if file_start.startswith(magic):
            return True  # can also return filetype

    fp_in.close()

    return False


# -----------------------------------------------------------------------------

if __name__ == '__main__':

    pass
